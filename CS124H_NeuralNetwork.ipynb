{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XqDwI3S3Cj_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbe0e18e-7ed7-4c3b-9005-802c47bbbc3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /mnt/drive; to attempt to forcibly remount, call drive.mount(\"/mnt/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/mnt/drive') # this loads a folder in my google drive\n",
        "# you cannot upload files to google colab permanently, so this is workaround"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !!!!!!!!!!!!!Madhav processing!!!!!!!!:\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "# Define the path to the training data\n",
        "data_dir = '/mnt/drive/MyDrive/datasets/data/training_data'\n",
        "\n",
        "# Initialize lists to hold data and labels\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "label_dirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
        "\n",
        "# Create a mapping from label to integer (optional, for numerical labels)\n",
        "label_to_int = {label: idx for idx, label in enumerate(sorted(label_dirs))}\n",
        "\n",
        "# Loop over each label directory\n",
        "for label in label_dirs:\n",
        "    label_dir = os.path.join(data_dir, label)\n",
        "    print(\"Currently on label \", label)\n",
        "    for image_name in os.listdir(label_dir):\n",
        "        image_path = os.path.join(label_dir, image_name)\n",
        "        if image_name.lower().endswith(('.png')):\n",
        "            try:\n",
        "                # Open the image and convert to grayscale\n",
        "                img = Image.open(image_path).convert('L')\n",
        "                x, y = img.size\n",
        "                if x != y:\n",
        "                    # Calculate padding amounts\n",
        "                    delta_w = max(x, y) - x\n",
        "                    delta_h = max(x, y) - y\n",
        "                    padding = (delta_w // 2, delta_h // 2, delta_w - (delta_w // 2), delta_h - (delta_h // 2))\n",
        "                    # Pad with white pixels (255)\n",
        "                    img = ImageOps.expand(img, padding, fill=255)\n",
        "                # Resize the image to 28x28 pixels\n",
        "                img = img.resize((28, 28))\n",
        "                img_array = np.array(img).flatten()\n",
        "                # Get the label as integer (or keep as string)\n",
        "                label_int = label_to_int[label]\n",
        "                # Append to data and labels\n",
        "                data.append(img_array)\n",
        "                labels.append(label_int)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {image_path}: {e}\")\n",
        "\n",
        "# Convert data and labels to pandas DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "df.insert(0, 'label', labels)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "# df.to_csv('processed_data.csv', index=False)\n",
        "df.to_csv('/mnt/drive/MyDrive/datasets/processed_training_data.csv', index=False)\n",
        "\n",
        "# Approximate 6 minutes ~"
      ],
      "metadata": {
        "id": "jRgPm8IApMJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image preprocessing\n",
        "train_directory = \"/mnt/drive/MyDrive/datasets/processed_training_data.csv\"\n",
        "# A = \"/mnt/drive/MyDrive/datasets/data/training_data/A\"\n",
        "\n",
        "# abandoned: \"/mnt/drive/MyDrive/datasets/emnist-letters-train.csv\""
      ],
      "metadata": {
        "id": "rKo0WTdZftju"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Import\n",
        "from sklearn.model_selection import train_test_split\n",
        "train = pd.read_csv(train_directory)  # this reads my dataset\n",
        "train.head() # prints the first rows of dataset\n",
        "train.iloc[:,0] # iloc functions slices the data\n",
        "# [;,0] is all the y values, labels\n",
        "# [:,1:] is all the pixel values, 28*28 = 784 columns of pixels\n",
        "x_train,x_val,y_train,y_val = train_test_split(train.iloc[1:,1:],train.iloc[1:,0],test_size=0.0005,random_state=42)\n",
        "# test_size is the size of the validation size relative to the whole dataset (here I use validation instead of test)\n",
        "# this splits data to training sets and validating sets(right now this is not used)."
      ],
      "metadata": {
        "id": "bOdQR2dOA_ze"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data processing\n",
        "x_train = x_train.values.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_val = x_val.values.reshape(x_val.shape[0], 28, 28, 1)\n",
        "# ensure the matrixes are [28][28][1]\n",
        "print(y_train.shape)\n",
        "y_train = keras.utils.to_categorical(y_train - 1, num_classes = 35)\n",
        "print(y_train.shape)\n",
        "# get_dummies do this, if your array is 1234, it returns 1 true 2 false 3 false 4 false for the first data, and the rest..\n",
        "y_val = keras.utils.to_categorical(y_val - 1, num_classes = 35)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "wLdCiG9KLmXP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b468bf99-6636-45fe-c61a-4e59c103b1ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(15992,)\n",
            "(15992, 35)\n",
            "(15992, 35)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Building\n",
        "model = keras.models.Sequential([\n",
        "\n",
        "    keras.layers.ZeroPadding2D(padding=(2, 2), input_shape=(28,28,1)),\n",
        "    keras.layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(32,32,1)),\n",
        "    keras.layers.AveragePooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'),\n",
        "    keras.layers.AveragePooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    keras.layers.Flatten(),\n",
        "\n",
        "    keras.layers.Dense(units=120, activation='relu'),\n",
        "    keras.layers.Dense(units=84, activation='relu'),\n",
        "    keras.layers.Dense(units=35, activation = 'softmax'),\n",
        "])\n",
        "\n",
        "# LeNet5 Model (can be remoduled)\n",
        "# However, input and out sizes must be the same"
      ],
      "metadata": {
        "id": "FDhL_6dhDSPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60838440-6dd6-4c8c-98db-4e6ab3fa0a7a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/zero_padding2d.py:72: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model compilation\n",
        "model.build() # builds the model\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# you can adjust any of the parameters, or even add metrics\n",
        "model.fit(x=x_train, y=y_train, epochs=15)\n",
        "# training the model, realisticly you would want to use more epoches than 5, for here I use fewer epoches for testing"
      ],
      "metadata": {
        "id": "cafRgxNSFrGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df6101db-8317-4e71-b302-46709d04a50f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - accuracy: 0.6021 - loss: 2.3703\n",
            "Epoch 2/15\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 20ms/step - accuracy: 0.9493 - loss: 0.1989\n",
            "Epoch 3/15\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - accuracy: 0.9673 - loss: 0.1164\n",
            "Epoch 4/15\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.9769 - loss: 0.0792\n",
            "Epoch 5/15\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - accuracy: 0.9816 - loss: 0.0616\n",
            "Epoch 6/15\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - accuracy: 0.9859 - loss: 0.0446\n",
            "Epoch 7/15\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - accuracy: 0.9848 - loss: 0.0437\n",
            "Epoch 8/15\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9903 - loss: 0.0301\n",
            "Epoch 9/15\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.9889 - loss: 0.0365\n",
            "Epoch 10/15\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - accuracy: 0.9922 - loss: 0.0228\n",
            "Epoch 11/15\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - accuracy: 0.9886 - loss: 0.0371\n",
            "Epoch 12/15\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - accuracy: 0.9941 - loss: 0.0193\n",
            "Epoch 13/15\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - accuracy: 0.9958 - loss: 0.0132\n",
            "Epoch 14/15\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 31ms/step - accuracy: 0.9901 - loss: 0.0298\n",
            "Epoch 15/15\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9941 - loss: 0.0216\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7901e4287f70>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the outcome\n",
        "x_test = pd.read_csv(train_directory) # I will change this later\n",
        "# x_test = pd.read_csv(\"/mnt/drive/MyDrive/datasets/emnist-letters-test.csv\")\n",
        "x_test.head()\n",
        "# read test dataset, or use your own images, anything\n",
        "\n",
        "x_test = x_test.iloc[:,1:]\n",
        "x_test = x_test.values.reshape(x_test.shape[0], 28, 28, 1)\n",
        "y_test = model.predict(x_test) # predicts y value\n",
        "y_test = np.argmax(y_test, axis=1) # after this line of code, you should get a character value\n",
        "y_test += 1\n",
        "print(y_test.max())\n",
        "print(y_test.min())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJyqYNEpN9m_",
        "outputId": "ad0d0db9-d9ef-4775-bc3f-83bb9dcb8a94"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m501/501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step\n",
            "35\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the outcome\n",
        "# y_test = np.array([chr(x + 97) for x in y_test]) # this maps integers to characters\n",
        "# out = pd.read_csv('/mnt/drive/MyDrive/datasets/out.csv')\n",
        "out = pd.DataFrame()\n",
        "out['Label'] = y_test\n",
        "out.to_csv('/mnt/drive/MyDrive/datasets/processed_test_data_out.csv', index=False)\n"
      ],
      "metadata": {
        "id": "KbZccWSgO6Om"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance Evaluation\n",
        "test = pd.read_csv(train_directory)\n",
        "test = test.iloc[:,0].values\n",
        "#test = np.array([chr(int(x) + 97) for x in test])\n",
        "\n",
        "matching_values = np.sum(test == y_test)\n",
        "total_values = y_test.size\n",
        "\n",
        "# Calculate the percentage\n",
        "percentage = (matching_values / total_values) * 100\n",
        "print(percentage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jfE6mRFl2KM",
        "outputId": "097204cb-f44c-4342-ff29-4bc058b56032"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99.69378827646544\n"
          ]
        }
      ]
    }
  ]
}